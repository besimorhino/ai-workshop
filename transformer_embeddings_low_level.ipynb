{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/besimorhino/ai-workshop/blob/main/transformer_embeddings_low_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2gvd-CwVPrT"
      },
      "source": [
        "# Transformers for Embeddings — A Low-Level, End-to-End Walkthrough\n",
        "\n",
        "Warning: This notebook is not for the faint of heart. It is designed to be **maximally instructive**, exposing every moving part in a transformer encoder used to produce **sentence embeddings**. It prioritizes **clarity and low-level details** over brevity or performance. Where helpful, we switch between **NumPy-only** (pure mechanics) and **PyTorch** (autograd + training) implementations.\n",
        "\n",
        "**What you'll build & see:**\n",
        "1. A tiny tokenizer & vocabulary; numericalization.\n",
        "2. Token embeddings and two positional encoding strategies (sinusoidal vs learned).\n",
        "3. Scaled dot-product attention step-by-step (explicit matrices, masking, softmax, weighted sums).\n",
        "4. Multi-head attention: head splitting/merging, per-head attention, and concatenation.\n",
        "5. LayerNorm and residual connections from scratch.\n",
        "6. Position-wise feed-forward network (FFN) with GELU.\n",
        "7. A full **TransformerEncoderLayer** (NumPy) and then a stack to form a **TransformerEncoder**.\n",
        "8. Producing **sentence embeddings** via [CLS] and mean pooling; cosine similarity search.\n",
        "9. (Optional) A **tiny PyTorch** encoder trained for a few iterations to illustrate learning dynamics and gradients.\n",
        "\n",
        "> ✨ Tip: Run cells in order. Wherever you see a `### STEP-BY-STEP` section, the cell prints intermediate tensors to demystify the math.\n"
      ],
      "id": "o2gvd-CwVPrT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRRw0yu4VPrV"
      },
      "source": [
        "\n",
        "## Background & Notation\n",
        "\n",
        "We focus on **encoder-only** transformers for embeddings. Let a tokenized sequence have length \\(L\\). The model dimension is \\(d_{\\text{model}}\\), attention key/query dimension is \\(d_k\\) (usually \\(d_{\\text{model}}/h\\)), and value dimension is \\(d_v\\) (usually \\(d_{\\text{model}}/h\\)), with \\(h\\) heads.\n",
        "\n",
        "**Scaled Dot-Product Attention:**\n",
        "Given \\(Q \\in \\mathbb{R}^{L \\times d_k}\\), \\(K \\in \\mathbb{R}^{L \\times d_k}\\), \\(V \\in \\mathbb{R}^{L \\times d_v}\\),\n",
        "\\[\n",
        "\\text{Attention}(Q,K,V) = \\text{Softmax}\\!\\Big(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\Big)V\n",
        "\\]\n",
        "where \\(M\\) is a mask with \\(-\\infty\\) at disallowed positions (or 0 if no mask).\n",
        "\n",
        "**Multi-Head Attention (MHA):**\n",
        "Project input \\(X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}\\) into per-head queries/keys/values, apply attention per head, then concatenate and re-project:\n",
        "\\[\n",
        "\\text{MHA}(X) = \\big[\\text{head}_1 \\| \\cdots \\| \\text{head}_h\\big]W_O\n",
        "\\]\n",
        "\n",
        "**Add & Norm (Pre-Norm style here):**\n",
        "We will use *pre-norm* blocks: \\(Y = X + \\text{SubLayer}(\\text{LayerNorm}(X))\\).\n",
        "\n",
        "**FFN:**\n",
        "\\[\n",
        "\\text{FFN}(x) = W_2\\,\\text{GELU}(W_1 x + b_1) + b_2\n",
        "\\]\n",
        "\n",
        "**Embeddings for sentences:**\n",
        "- **[CLS]**: take the embedding corresponding to the special classification token at position 0.\n",
        "- **Mean pooling**: average the token embeddings (optionally excluding padding and special tokens), then \\(\\ell_2\\)-normalize.\n"
      ],
      "id": "PRRw0yu4VPrV"
    },
    {
      "cell_type": "code",
      "source": [
        "# \"OPTIONAL\" CELL!\n",
        "# in Google colab you _should_ be able to skip this cell.\n",
        "# this step is included for those who wish to run the workbook locally.\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "cb5BqtshXQa3"
      },
      "id": "cb5BqtshXQa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha3JUACpVPrV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Core imports\n",
        "import math, random, string, itertools, functools, types\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Try torch for the optional training section\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    TORCH_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    TORCH_AVAILABLE = False\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "random.seed(7)\n",
        "np.random.seed(7)\n",
        "\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"PyTorch available:\", TORCH_AVAILABLE)\n"
      ],
      "id": "Ha3JUACpVPrV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you got a message about torch not being availble, please go back and run the pip install cell earlier in the workbook"
      ],
      "metadata": {
        "id": "LNEtrDllXtmi"
      },
      "id": "LNEtrDllXtmi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFu2BeN2VPrW"
      },
      "source": [
        "\n",
        "## 1) Tiny Tokenizer & Vocabulary\n",
        "\n",
        "We'll implement a super-simple tokenizer:\n",
        "- Lowercase text\n",
        "- Split on spaces and punctuation (keeping punctuation as separate tokens)\n",
        "- Add special tokens: `[PAD]`, `[UNK]`, `[BOS]`, `[EOS]`, `[CLS]`\n",
        "\n",
        "> This is **not** a production tokenizer. It's intentionally simple to expose mechanics.\n"
      ],
      "id": "hFu2BeN2VPrW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN4d1FvkVPrW"
      },
      "outputs": [],
      "source": [
        "\n",
        "SPECIALS = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"[CLS]\"]\n",
        "PAD, UNK, BOS, EOS, CLS = SPECIALS\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    text = text.lower().strip()\n",
        "    # Separate punctuation by spacing it out\n",
        "    punct = set(list(\".,!?;:()[]{}'\\\"-\"))\n",
        "    spaced = []\n",
        "    for ch in text:\n",
        "        if ch in punct:\n",
        "            spaced.extend([\" \", ch, \" \"])\n",
        "        else:\n",
        "            spaced.append(ch)\n",
        "    text = \"\".join(spaced)\n",
        "    # Collapse whitespace\n",
        "    toks = [t for t in text.split() if t]\n",
        "    return toks\n",
        "\n",
        "corpus = [\n",
        "    \"Transformers map sequences to sequences using attention.\",\n",
        "    \"We will build a tiny encoder to learn embeddings.\",\n",
        "    \"Attention lets each token attend to others.\",\n",
        "    \"Embeddings capture semantic content of sentences.\",\n",
        "    \"Mean pooling and CLS pooling are common strategies.\",\n",
        "    \"Cosine similarity compares sentence embeddings.\",\n",
        "]\n",
        "\n",
        "# Build vocabulary from corpus\n",
        "tok_counts = Counter()\n",
        "for s in corpus:\n",
        "    tok_counts.update(simple_tokenize(s))\n",
        "\n",
        "vocab = SPECIALS + sorted(tok_counts.keys())\n",
        "stoi = {t:i for i,t in enumerate(vocab)}\n",
        "itos = {i:t for t,i in stoi.items()}\n",
        "\n",
        "def encode(text, add_specials=True, max_len=None):\n",
        "    toks = simple_tokenize(text)\n",
        "    if add_specials:\n",
        "        toks = [CLS, BOS] + toks + [EOS]\n",
        "    ids = [stoi.get(t, stoi[UNK]) for t in toks]\n",
        "    if max_len is not None:\n",
        "        ids = ids[:max_len] + [stoi[PAD]] * max(0, max_len - len(ids))\n",
        "    return np.array(ids, dtype=np.int64)\n",
        "\n",
        "def decode(ids):\n",
        "    toks = [itos.get(int(i), UNK) for i in ids]\n",
        "    # remove PADs for readability\n",
        "    toks = [t for t in toks if t != PAD]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"Sample tokens:\", vocab[:25])\n",
        "\n",
        "ex = encode(\"Transformers are amazing!\", add_specials=True, max_len=16)\n",
        "print(\"Encoded:\", ex)\n",
        "print(\"Decoded:\", decode(ex))\n"
      ],
      "id": "JN4d1FvkVPrW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DapYZ719VPrW"
      },
      "source": [
        "\n",
        "## 2) Positional Encodings\n",
        "\n",
        "We demonstrate two variants:\n",
        "\n",
        "- **Sinusoidal** (deterministic, no parameters):\n",
        "  \\[ PE_{(pos,2i)} = \\sin\\big(pos / 10000^{2i/d_{model}}\\big), \\quad\n",
        "     PE_{(pos,2i+1)} = \\cos\\big(pos / 10000^{2i/d_{model}}\\big) \\]\n",
        "\n",
        "- **Learned positional embeddings**: a trainable matrix \\(P \\in \\mathbb{R}^{L_\\text{max} \\times d_{\\text{model}}}\\).\n",
        "\n",
        "We also define **token embeddings** \\(E \\in \\mathbb{R}^{|V| \\times d_{\\text{model}}}\\) and combine as \\(X = E[\\text{tokens}] + P[\\text{positions}]\\).\n"
      ],
      "id": "DapYZ719VPrW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM7ceLnZVPrW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sinusoidal_positional_encoding(max_len, d_model):\n",
        "    pos = np.arange(max_len)[:, None]  # (L,1)\n",
        "    i = np.arange(d_model)[None, :]    # (1,d)\n",
        "    angle_rates = 1 / np.power(10000, (2*(i//2))/np.float32(d_model))\n",
        "    angles = pos * angle_rates\n",
        "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
        "    pe[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    pe[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    return pe  # (L,d)\n",
        "\n",
        "class LearnedPositionalEncoding:\n",
        "    def __init__(self, max_len, d_model, rng=np.random.default_rng(7)):\n",
        "        self.table = rng.normal(0.0, 0.02, size=(max_len, d_model)).astype(np.float32)\n",
        "    def __call__(self, positions):\n",
        "        return self.table[positions]\n",
        "\n",
        "# Token embedding matrix\n",
        "def make_token_embedding(vocab_size, d_model, rng=np.random.default_rng(7)):\n",
        "    return rng.normal(0.0, 0.02, size=(vocab_size, d_model)).astype(np.float32)\n",
        "\n",
        "# Visualize sinusoidal patterns\n",
        "max_len, d_model = 64, 32\n",
        "pe = sinusoidal_positional_encoding(max_len, d_model)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.imshow(pe[:64, :32])\n",
        "plt.title(\"Sinusoidal Positional Encoding (first 64x32)\")\n",
        "plt.xlabel(\"d_model dim\")\n",
        "plt.ylabel(\"position\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Compose embeddings\n",
        "vocab_size = len(vocab)\n",
        "E = make_token_embedding(vocab_size, d_model)\n",
        "positions = np.arange(16)\n",
        "LP = LearnedPositionalEncoding(512, d_model)\n",
        "\n",
        "sample_ids = encode(\"attention mechanisms are cool.\", add_specials=True, max_len=16)\n",
        "X_sinus = E[sample_ids] + pe[positions]\n",
        "X_learn = E[sample_ids] + LP(positions)\n",
        "\n",
        "print(\"X_sinus shape:\", X_sinus.shape, \"| X_learn shape:\", X_learn.shape)\n"
      ],
      "id": "ZM7ceLnZVPrW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFVF61coVPrX"
      },
      "source": [
        "\n",
        "## 3) Scaled Dot-Product Attention — **STEP-BY-STEP**\n",
        "\n",
        "We construct \\(Q= XW_Q\\), \\(K= XW_K\\), \\(V = XW_V\\) and compute attention explicitly. We'll use a small \\(d_{\\text{model}}\\) to print matrices.\n",
        "\n",
        "Masking option: we'll show a padding mask that prevents attending to PAD tokens.\n"
      ],
      "id": "eFVF61coVPrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB4pZiVCVPrX"
      },
      "outputs": [],
      "source": [
        "def softmax(x, axis=-1):\n",
        "    x = x - np.max(x, axis=axis, keepdims=True)\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "def make_qkv(X, d_model_in, d_k, d_v, rng=np.random.default_rng(7)):\n",
        "    W_Q = rng.normal(0, 0.02, size=(d_model_in, d_k)).astype(np.float32)\n",
        "    W_K = rng.normal(0, 0.02, size=(d_model_in, d_k)).astype(np.float32)\n",
        "    W_V = rng.normal(0, 0.02, size=(d_model_in, d_v)).astype(np.float32)\n",
        "    Q = X @ W_Q\n",
        "    K = X @ W_K\n",
        "    V = X @ W_V\n",
        "    return Q, K, V, W_Q, W_K, W_V\n",
        "\n",
        "def attention(Q, K, V, mask=None):\n",
        "    # Q: (L,d_k), K: (L,d_k), V: (L,d_v)\n",
        "    scores = (Q @ K.T) / math.sqrt(Q.shape[-1])  # (L,L)\n",
        "    if mask is not None:\n",
        "        scores = scores + mask  # mask should be 0 or -inf at invalid positions\n",
        "    weights = softmax(scores, axis=-1)           # (L,L)\n",
        "    out = weights @ V                             # (L,d_v)\n",
        "    return out, weights, scores\n",
        "\n",
        "# Prepare a tiny example\n",
        "L = 6; d_model = 32; num_heads = 4; d_k = d_model // num_heads; d_v = d_model // num_heads\n",
        "ids = encode(\"Transformers map sequences to sequences using attention.\", add_specials=True, max_len=L)\n",
        "pe_slice = sinusoidal_positional_encoding(max_len=L, d_model=d_model)\n",
        "X = (E[ids] + pe_slice)  # (L,d_model)\n",
        "\n",
        "Q, K, V, WQ, WK, WV = make_qkv(X, d_model, d_k, d_v)\n",
        "\n",
        "# Build a padding mask: 0 for valid, -1e9 for PAD positions\n",
        "pad_mask = (ids == stoi[PAD]).astype(np.float32)\n",
        "# Convert to additive mask over keys: (L,L), mask rows for each query over every key position\n",
        "additive_mask = np.where(pad_mask[None, :]==1, -1e9, 0.0).astype(np.float32)\n",
        "\n",
        "out, weights, scores = attention(Q, K, V, mask=additive_mask)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"Q/K/V shapes:\", Q.shape, K.shape, V.shape)\n",
        "print(\"Raw scores (pre-softmax):\\n\", np.round(scores, 4))\n",
        "print(\"Attention weights (rows sum to 1):\\n\", np.round(weights, 4))\n",
        "print(\"Output (weighted sums):\\n\", np.round(out, 4))\n",
        "\n",
        "# Visualize attention weights\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(weights)\n",
        "plt.title(\"Attention Weights\")\n",
        "plt.xlabel(\"Key j\")\n",
        "plt.ylabel(\"Query i\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "XB4pZiVCVPrX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqwjIjmXVPrX"
      },
      "source": [
        "\n",
        "## 4) Multi-Head Attention (MHA)\n",
        "\n",
        "We split \\(d_{\\text{model}}\\) into \\(h\\) heads of size \\(d_k=d_v=d_{\\text{model}}/h\\). For each head \\(i\\), compute attention independently and then concatenate results. Finally apply an output projection \\(W_O\\).\n"
      ],
      "id": "sqwjIjmXVPrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsJTrbinVPrX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttentionNumpy:\n",
        "    def __init__(self, d_model, num_heads, rng=np.random.default_rng(7)):\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.h = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_v = self.d_k\n",
        "\n",
        "        # Combine projections in single big matrices for convenience\n",
        "        self.W_Q = rng.normal(0, 0.02, size=(d_model, d_model)).astype(np.float32)\n",
        "        self.W_K = rng.normal(0, 0.02, size=(d_model, d_model)).astype(np.float32)\n",
        "        self.W_V = rng.normal(0, 0.02, size=(d_model, d_model)).astype(np.float32)\n",
        "        self.W_O = rng.normal(0, 0.02, size=(d_model, d_model)).astype(np.float32)\n",
        "\n",
        "    def _split_heads(self, X):\n",
        "        # X: (L, d_model) -> (h, L, d_k)\n",
        "        L = X.shape[0]\n",
        "        Xh = X.reshape(L, self.h, self.d_k).transpose(1,0,2)\n",
        "        return Xh\n",
        "\n",
        "    def _merge_heads(self, Xh):\n",
        "        # Xh: (h, L, d_v) -> (L, d_model)\n",
        "        h, L, dk = Xh.shape\n",
        "        return Xh.transpose(1,0,2).reshape(L, h*dk)\n",
        "\n",
        "    def __call__(self, X, additive_mask=None, return_weights=False):\n",
        "        # Project\n",
        "        Q = X @ self.W_Q  # (L,d_model)\n",
        "        K = X @ self.W_K\n",
        "        V = X @ self.W_V\n",
        "        # Split heads\n",
        "        Qh, Kh, Vh = self._split_heads(Q), self._split_heads(K), self._split_heads(V)\n",
        "        outputs = []\n",
        "        all_weights = []\n",
        "        for i in range(self.h):\n",
        "            out, w, _ = attention(Qh[i], Kh[i], Vh[i], mask=additive_mask)\n",
        "            outputs.append(out[None, ...])   # (1,L,d_k)\n",
        "            all_weights.append(w[None, ...]) # (1,L,L)\n",
        "        H = np.concatenate(outputs, axis=0)      # (h,L,d_k)\n",
        "        concat = self._merge_heads(H)            # (L,d_model)\n",
        "        Y = concat @ self.W_O                    # (L,d_model)\n",
        "        if return_weights:\n",
        "            W = np.concatenate(all_weights, axis=0) # (h,L,L)\n",
        "            return Y, W\n",
        "        return Y\n",
        "\n",
        "# Demo\n",
        "mha = MultiHeadAttentionNumpy(d_model=32, num_heads=4)\n",
        "ids = encode(\"embeddings capture semantic content of sentences.\", add_specials=True, max_len=10)\n",
        "X = E[ids] + pe[:len(ids)]\n",
        "pad_mask = (ids == stoi[PAD]).astype(np.float32)\n",
        "add_mask = np.where(pad_mask[None, :]==1, -1e9, 0.0).astype(np.float32)\n",
        "\n",
        "Y, W = mha(X, additive_mask=add_mask, return_weights=True)\n",
        "print(\"MHA output shape:\", Y.shape, \"| attn weights shape:\", W.shape)\n",
        "\n",
        "# Visualize head 0 weights\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(W[0])\n",
        "plt.title(\"Head 0 Attention Weights\")\n",
        "plt.xlabel(\"Key j\")\n",
        "plt.ylabel(\"Query i\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "id": "NsJTrbinVPrX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cez9_dkgVPrX"
      },
      "source": [
        "\n",
        "## 5) LayerNorm & Residuals\n",
        "\n",
        "We implement LayerNorm from scratch with epsilon for numerical stability. We'll use **pre-norm** blocks:\n",
        "- \\(Z = X + \\text{SubLayer}(\\text{LayerNorm}(X))\\)\n"
      ],
      "id": "Cez9_dkgVPrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HPpgs8sVPrX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LayerNormNumpy:\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        self.gamma = np.ones((d_model,), dtype=np.float32)\n",
        "        self.beta  = np.zeros((d_model,), dtype=np.float32)\n",
        "        self.eps = eps\n",
        "    def __call__(self, X):\n",
        "        # X: (L, d_model)\n",
        "        mu = X.mean(axis=-1, keepdims=True)\n",
        "        var = ((X - mu)**2).mean(axis=-1, keepdims=True)\n",
        "        Xhat = (X - mu) / np.sqrt(var + self.eps)\n",
        "        return self.gamma * Xhat + self.beta\n",
        "\n",
        "# Quick test\n",
        "X = np.random.randn(5, 32).astype(np.float32)\n",
        "ln = LayerNormNumpy(32)\n",
        "Y = ln(X)\n",
        "print(\"LayerNorm ok, mean~\", Y.mean(), \"std~\", Y.std())\n"
      ],
      "id": "8HPpgs8sVPrX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTi6hX5HVPrX"
      },
      "source": [
        "\n",
        "## 6) Position-wise Feed-Forward Network (FFN)\n",
        "\n",
        "Two linear layers with a nonlinearity (GELU). Implemented from scratch with NumPy.\n"
      ],
      "id": "mTi6hX5HVPrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7DhZoeMVPrX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gelu(x):\n",
        "    # Approximate GELU\n",
        "    return 0.5 * x * (1.0 + np.tanh(np.sqrt(2.0/np.pi) * (x + 0.044715 * (x**3))))\n",
        "\n",
        "class FFNNumpy:\n",
        "    def __init__(self, d_model, d_hidden, rng=np.random.default_rng(7)):\n",
        "        self.W1 = rng.normal(0, 0.02, size=(d_model, d_hidden)).astype(np.float32)\n",
        "        self.b1 = np.zeros((d_hidden,), dtype=np.float32)\n",
        "        self.W2 = rng.normal(0, 0.02, size=(d_hidden, d_model)).astype(np.float32)\n",
        "        self.b2 = np.zeros((d_model,), dtype=np.float32)\n",
        "    def __call__(self, X):\n",
        "        return (gelu(X @ self.W1 + self.b1)) @ self.W2 + self.b2\n",
        "\n",
        "# Quick test\n",
        "ff = FFNNumpy(32, 64)\n",
        "X = np.random.randn(7, 32).astype(np.float32)\n",
        "print(\"FFN out shape:\", ff(X).shape)\n"
      ],
      "id": "U7DhZoeMVPrX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khRLekDfVPrX"
      },
      "source": [
        "\n",
        "## 7) TransformerEncoderLayer (NumPy, Pre-Norm)\n",
        "\n",
        "Combine: LayerNorm → MHA → Residual → LayerNorm → FFN → Residual.\n"
      ],
      "id": "khRLekDfVPrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSfOIdYiVPrX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerEncoderLayerNumpy:\n",
        "    def __init__(self, d_model=64, num_heads=4, d_hidden=128, rng=np.random.default_rng(7)):\n",
        "        self.ln1 = LayerNormNumpy(d_model)\n",
        "        self.ln2 = LayerNormNumpy(d_model)\n",
        "        self.mha = MultiHeadAttentionNumpy(d_model, num_heads, rng=rng)\n",
        "        self.ffn = FFNNumpy(d_model, d_hidden, rng=rng)\n",
        "    def __call__(self, X, additive_mask=None, return_attn=False):\n",
        "        H = self.ln1(X)\n",
        "        Hm, W = self.mha(H, additive_mask=additive_mask, return_weights=True)\n",
        "        X = X + Hm\n",
        "        H2 = self.ln2(X)\n",
        "        X = X + self.ffn(H2)\n",
        "        if return_attn:\n",
        "            return X, W\n",
        "        return X\n",
        "\n",
        "class TransformerEncoderNumpy:\n",
        "    def __init__(self, num_layers, d_model=64, num_heads=4, d_hidden=128, rng=np.random.default_rng(7)):\n",
        "        self.layers = [TransformerEncoderLayerNumpy(d_model, num_heads, d_hidden, rng=rng) for _ in range(num_layers)]\n",
        "        self.d_model = d_model\n",
        "    def __call__(self, X, additive_mask=None, return_all=False):\n",
        "        attns = []\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            X, W = layer(X, additive_mask=additive_mask, return_attn=True)\n",
        "            attns.append(W)\n",
        "        if return_all:\n",
        "            return X, attns\n",
        "        return X\n",
        "\n",
        "# Demo end-to-end\n",
        "d_model = 64; num_heads=4; d_hidden=128; num_layers=2\n",
        "max_len = 24\n",
        "\n",
        "E = make_token_embedding(len(vocab), d_model)\n",
        "pe = sinusoidal_positional_encoding(max_len, d_model)\n",
        "\n",
        "sent = \"Mean pooling and CLS pooling are common strategies.\"\n",
        "ids = encode(sent, add_specials=True, max_len=max_len)\n",
        "X0 = E[ids] + pe[:len(ids)]\n",
        "pad_mask = (ids == stoi[PAD]).astype(np.float32)\n",
        "add_mask = np.where(pad_mask[None, :]==1, -1e9, 0.0).astype(np.float32)\n",
        "\n",
        "encoder = TransformerEncoderNumpy(num_layers, d_model, num_heads, d_hidden)\n",
        "Z, attn_list = encoder(X0, additive_mask=add_mask, return_all=True)\n",
        "\n",
        "print(\"Encoder output shape:\", Z.shape)\n",
        "print(\"Attn heads per layer:\", [A.shape for A in attn_list])  # each (h,L,L)\n"
      ],
      "id": "PSfOIdYiVPrX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBzZrFNtVPrY"
      },
      "source": [
        "\n",
        "## 8) Sentence Embeddings: [CLS] vs Mean Pool\n",
        "\n",
        "We derive fixed-length vectors from token-level outputs:\n",
        "- **CLS pooling**: take vector at position of `[CLS]` (index 0 if we prepended it).\n",
        "- **Mean pooling**: average token vectors over non-padding positions (optionally ignore specials).\n",
        "Finally, L2-normalize for cosine similarity.\n"
      ],
      "id": "vBzZrFNtVPrY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iIJXpKrVPrY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def l2_normalize(x, eps=1e-12):\n",
        "    nrm = np.sqrt((x**2).sum(-1, keepdims=True))\n",
        "    return x / np.clip(nrm, eps, None)\n",
        "\n",
        "def get_sentence_embedding(Z, ids, method=\"mean\", ignore_specials=True):\n",
        "    # Z: (L, d_model), ids: (L,)\n",
        "    if method == \"cls\":\n",
        "        # CLS is the very first token if we added [CLS], [BOS], ...\n",
        "        idx = 0\n",
        "        v = Z[idx]\n",
        "        return l2_normalize(v[None, :])[0]\n",
        "    elif method == \"mean\":\n",
        "        mask = (ids != stoi[PAD]).astype(np.float32)\n",
        "        if ignore_specials:\n",
        "            specials = {stoi[t] for t in SPECIALS}\n",
        "            mask = mask * np.array([0.0 if int(i) in specials else 1.0 for i in ids], dtype=np.float32)\n",
        "        denom = max(1.0, mask.sum())\n",
        "        v = (Z * mask[:, None]).sum(axis=0) / denom\n",
        "        return l2_normalize(v[None, :])[0]\n",
        "    else:\n",
        "        raise ValueError(\"Unknown method\")\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12))\n",
        "\n",
        "# Build a small set and compare\n",
        "sentences = [\n",
        "    \"A transformer encoder builds contextual token embeddings.\",\n",
        "    \"We compute mean pooled sentence vectors for similarity.\",\n",
        "    \"Fluffy cats sleep in sunny windows.\",\n",
        "    \"Attention allows tokens to interact across positions.\",\n",
        "    \"The weather is rainy, bring an umbrella.\",\n",
        "    \"CLS pooling extracts the first token representation.\",\n",
        "]\n",
        "\n",
        "def embed_sentence(encoder, E, pe, text, max_len=24, method=\"mean\"):\n",
        "    ids = encode(text, add_specials=True, max_len=max_len)\n",
        "    X = E[ids] + pe[:len(ids)]\n",
        "    pad_mask = (ids == stoi[PAD]).astype(np.float32)\n",
        "    add_mask = np.where(pad_mask[None, :]==1, -1e9, 0.0).astype(np.float32)\n",
        "    Z = encoder(X, additive_mask=add_mask)\n",
        "    return get_sentence_embedding(Z, ids, method=method), ids, Z\n",
        "\n",
        "# Reuse encoder from above (random weights => not trained)\n",
        "method = \"mean\"  # try \"cls\" as well\n",
        "vecs = []\n",
        "for s in sentences:\n",
        "    v, ids_s, Zs = embed_sentence(encoder, E, pe, s, method=method)\n",
        "    vecs.append(v)\n",
        "\n",
        "print(\"Pairwise cosine similarities:\")\n",
        "for i in range(len(sentences)):\n",
        "    row = []\n",
        "    for j in range(len(sentences)):\n",
        "        row.append(f\"{cosine_sim(vecs[i], vecs[j]): .3f}\")\n",
        "    print(i, row)\n",
        "\n",
        "# Visualize attention of first layer, head 0 for a sample sentence\n",
        "ids = encode(sentences[0], add_specials=True, max_len=24)\n",
        "X = E[ids] + pe[:len(ids)]\n",
        "pad_mask = (ids == stoi[PAD]).astype(np.float32)\n",
        "add_mask = np.where(pad_mask[None, :]==1, -1e9, 0.0).astype(np.float32)\n",
        "Z, attn_all = encoder(X, additive_mask=add_mask, return_all=True)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(attn_all[0][0])  # layer 0, head 0\n",
        "plt.title(\"Layer 0, Head 0 Attention\")\n",
        "plt.xlabel(\"Key j\")\n",
        "plt.ylabel(\"Query i\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "id": "8iIJXpKrVPrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu70CIohVPrY"
      },
      "source": [
        "\n",
        "## 9) (Optional) Tiny PyTorch Encoder + Quick Training\n",
        "\n",
        "If PyTorch is available, we'll build a small encoder and train for a few hundred steps on a toy objective:\n",
        "- **Next-token prediction** (causal-ish over our **encoder** for demo) or\n",
        "- **Denoising** (mask a token and predict it).\n",
        "\n",
        "This is just to show **gradients**, **loss decreasing**, and how embeddings become more meaningful.\n"
      ],
      "id": "zu70CIohVPrY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bucwcJ6VPrY"
      },
      "outputs": [],
      "source": [
        "\n",
        "if TORCH_AVAILABLE:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Torch versions of token & positional embeddings\n",
        "    class TorchSinusoidalPositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, max_len, d_model):\n",
        "            super().__init__()\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            self.register_buffer('pe', pe)  # (max_len, d_model)\n",
        "\n",
        "        def forward(self, positions):\n",
        "            return self.pe[positions]\n",
        "\n",
        "    class TinyEncoder(nn.Module):\n",
        "        def __init__(self, vocab_size, d_model=64, num_heads=4, d_hidden=128, num_layers=2, max_len=64):\n",
        "            super().__init__()\n",
        "            self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "            self.pos_enc = TorchSinusoidalPositionalEncoding(max_len, d_model)\n",
        "            enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_hidden, batch_first=True, norm_first=True, activation=\"gelu\")\n",
        "            self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "            self.lm_head = nn.Linear(d_model, vocab_size)  # for next-token prediction\n",
        "            self.max_len = max_len\n",
        "            self.d_model = d_model\n",
        "\n",
        "        def forward(self, input_ids, pad_id):\n",
        "            # input_ids: (B, L)\n",
        "            B, L = input_ids.shape\n",
        "            positions = torch.arange(L, device=input_ids.device)\n",
        "            X = self.token_emb(input_ids) + self.pos_enc(positions)\n",
        "            key_padding_mask = (input_ids == pad_id)  # (B, L) True at pads\n",
        "            Z = self.encoder(X, src_key_padding_mask=key_padding_mask)\n",
        "            logits = self.lm_head(Z)  # (B, L, V)\n",
        "            return Z, logits\n",
        "\n",
        "    # Build dataset (toy): sequences from our small corpus\n",
        "    def batchify(texts, max_len=24, batch_size=8):\n",
        "        ids = [encode(t, add_specials=True, max_len=max_len) for t in texts]\n",
        "        arr = torch.tensor(np.stack(ids), dtype=torch.long)\n",
        "        # simple batching by repeat + shuffle\n",
        "        reps = 64 // len(texts) + 1\n",
        "        data = arr.repeat((reps, 1))\n",
        "        idx = torch.randperm(data.size(0))\n",
        "        data = data[idx]\n",
        "        # chunk into batches\n",
        "        for i in range(0, data.size(0), batch_size):\n",
        "            yield data[i:i+batch_size]\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "    pad_id = stoi[PAD]\n",
        "    model = TinyEncoder(vocab_size).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    # Quick training loop: next-token prediction (shift inputs by 1)\n",
        "    steps, log_every = 200, 40\n",
        "    model.train()\n",
        "    for step, batch in enumerate(itertools.islice(batchify(corpus, batch_size=16), steps)):\n",
        "        batch = batch.to(device)\n",
        "        Z, logits = model(batch, pad_id=pad_id) # (B, L, V)\n",
        "        # targets are next tokens (right-shift)\n",
        "        targets = batch[:, 1:].contiguous()\n",
        "        preds = logits[:, :-1, :].contiguous()  # align\n",
        "        loss = loss_fn(preds.view(-1, vocab_size), targets.view(-1))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if (step+1) % log_every == 0:\n",
        "            print(f\"step {step+1:4d} | loss {loss.item():.4f}\")\n",
        "\n",
        "    # Extract embeddings and compare similarities post-training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        def torch_embed_sentence(text, method=\"mean\"):\n",
        "            ids_np = encode(text, add_specials=True, max_len=24)\n",
        "            ids_t = torch.tensor(ids_np)[None, :].to(device)\n",
        "            Z, _ = model(ids_t, pad_id=pad_id)  # (1, L, d)\n",
        "            Z = Z[0]  # (L, d)\n",
        "            if method == \"cls\":\n",
        "                v = Z[0]\n",
        "            else:\n",
        "                mask = (ids_t[0] != pad_id).float()\n",
        "                # ignore specials\n",
        "                specials = [stoi[s] for s in SPECIALS]\n",
        "                special_mask = torch.ones_like(mask)\n",
        "                for s in specials:\n",
        "                    special_mask = special_mask * (ids_t[0] != s).float()\n",
        "                mask = mask * special_mask\n",
        "                denom = torch.clamp(mask.sum(), min=1.0)\n",
        "                v = (Z * mask[:, None]).sum(dim=0) / denom\n",
        "            v = F.normalize(v, dim=0)\n",
        "            return v.cpu().numpy()\n",
        "\n",
        "        v_train = [torch_embed_sentence(s, method=\"mean\") for s in sentences]\n",
        "        print(\"Pairwise cosine similarities after quick training:\")\n",
        "        for i in range(len(sentences)):\n",
        "            row = []\n",
        "            for j in range(len(sentences)):\n",
        "                row.append(f\"{float(np.dot(v_train[i], v_train[j])): .3f}\")\n",
        "            print(i, row)\n",
        "else:\n",
        "    print(\"PyTorch not available; skipping training section.\")\n"
      ],
      "id": "2bucwcJ6VPrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFzixXlpVPrY"
      },
      "source": [
        "\n",
        "## 10) Exercises & Further Work\n",
        "\n",
        "1. **Pooling variants:** Try max-pooling, attention-pooling (learn a query vector), or combinations.\n",
        "2. **Masking:** Implement a causal mask and compare to padding mask effects on attention patterns.\n",
        "3. **Normalization:** Switch to *post-norm* (apply LayerNorm after residual) and observe training stability in Torch section.\n",
        "4. **Dimensionality:** Increase `d_model`, `num_heads`, and `num_layers`; check how attention maps change.\n",
        "5. **Objective:** Replace next-token with a denoising (masking) objective; compare the quality of embeddings (cosine clusters).\n",
        "6. **Whitening & anisotropy:** After mean pooling, try whitening or a root-mean-square normalization on embeddings and evaluate cosine similarities.\n",
        "7. **Visualization:** Log attention across layers and heads for multiple sentences; build an attention rollout visualization.\n"
      ],
      "id": "pFzixXlpVPrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n464gQ7VPrY"
      },
      "source": [
        "\n",
        "### Some Tips:\n",
        "- If you have a CUDA supported hardware, it may make sense to run this workbook on that hardware instead of the free tier of Google Colab.\n",
        "- If you still want to run this in Google Colab, you may want to consider changing the runtime type → (optional) GPU for the tiny Torch training speedup.\n",
        "- If you see `PyTorch not available; skipping training section.`, ensure Torch is installed in your Colab runtime (it typically is). If it's not be sure to run `!pip install torch`\n",
        "\n"
      ],
      "id": "5n464gQ7VPrY"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8VIvhR4W7l7"
      },
      "id": "A8VIvhR4W7l7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}