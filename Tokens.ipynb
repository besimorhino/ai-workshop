{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfBhH+1tsYIRzN2+DDJOfa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/besimorhino/ai-workshop/blob/main/Tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0. Setup\n",
        "\n",
        "We need to install some python libraries to make our demo code work."
      ],
      "metadata": {
        "id": "s3BjtNM9aS4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community"
      ],
      "metadata": {
        "id": "U1Qp61T6Wq9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load the librarires needed to generate our tokens"
      ],
      "metadata": {
        "id": "_i4Ec2_KaYB3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlwl-v1sVaY6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Tokenizing\n",
        "Tokenizing is a fancy term for slicing up something (in this case text) and representing it in another way. It is a foundational requirement for AI systems being able to understand human text. This area of computer science is more commonly called Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "dCv8mf05aeCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been wanting to know what tokens are all about...\",\n",
        "    \"Do they live up to the hype?\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "VADe4IZ6VcAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go for a much larger data set!"
      ],
      "metadata": {
        "id": "bprKdx1hWPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This downloads State of the Union speech data\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"./data/state_of_the_union.txt\") == False:\n",
        "    !mkdir ./data\n",
        "    !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt"
      ],
      "metadata": {
        "id": "hLJzTufuWTOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TextLoader function\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load the documents we want to prompt an LLM about\n",
        "doc = TextLoader(\"data/state_of_the_union.txt\").load()"
      ],
      "metadata": {
        "id": "jYtSzOEfWWBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see that we have 1 document loaded in the doc variable"
      ],
      "metadata": {
        "id": "zKvO56OGc5Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "id": "VhlgH7vccwBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk the documents into 200 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "CioPXjwTXO0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_documents produces a list of all the chunks created\n",
        "chunks = [p.page_content for p in text_splitter.split_documents(doc)]"
      ],
      "metadata": {
        "id": "KElCeEMuW5Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The single document is now split into many chunks"
      ],
      "metadata": {
        "id": "zqycOO-RdFjJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4459aed8"
      },
      "source": [
        "len(chunks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_chunks = []\n",
        "for chunk in chunks:\n",
        "  inputs = tokenizer(chunk, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "  tokenized_chunks.append(inputs)"
      ],
      "metadata": {
        "id": "bbi36zSnbyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show first chunk of State of Union speech"
      ],
      "metadata": {
        "id": "PnNe-t4jW7HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "id": "ujb-b-bYW52g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the token that was made from this chunk."
      ],
      "metadata": {
        "id": "0ycOqA_3cgEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_chunks[0]"
      ],
      "metadata": {
        "id": "a-I1H12sckkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting detail: Unless you force it to split at a specific boundary, the `RecursiveCharacterTextSplitter` class from LangChain attempts to split at an 'acceptable' boundary. This results in chunks that are about the size of our chunksize.\n",
        "\n",
        "Here we see the length of the first chunk"
      ],
      "metadata": {
        "id": "zni9cAsVdXpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks[0])"
      ],
      "metadata": {
        "id": "etzcpN1udbl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But you can see that not all the chunks are the same size!"
      ],
      "metadata": {
        "id": "2UKBuvEgelO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "  print(len(chunk))"
      ],
      "metadata": {
        "id": "-MVpRn0zep_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}