{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbnzDJJbrCNxnYmixexS5l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/besimorhino/ai-workshop/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About This Lab\n",
        "In this workbook we will explore how generative AI can be used to complete fragments of text. This is helpful in many situations such as audio transcription where an LLM enabled system may needs to make guesses on garbled speech."
      ],
      "metadata": {
        "id": "b1C7g7D9xD6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0. Setup\n",
        "\n",
        "We need to install some python libraries to make our demo code work."
      ],
      "metadata": {
        "id": "SXz8wZ-Ab4Mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yoNrau6j__-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we import libraries from the code we just installed."
      ],
      "metadata": {
        "id": "X-T4_hWtcEHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "7mFE7CnikDYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Generating text\n",
        "Generative AI will use token statistical probabilities to output text that is most likely to follow given the prompt.\n",
        "\n",
        "Note: the model used will have MASSIVE impacts on how well the generated text \"flows\". Larger models tend to have more 'natural' output, but are much more resource intense and expensive."
      ],
      "metadata": {
        "id": "W5OZ7Tb4cHC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this lab, we will teach you how to\")"
      ],
      "metadata": {
        "id": "HZTrQjCbkF8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"Some important things to remember about OPSEC are\")"
      ],
      "metadata": {
        "id": "pyMFIvfakL_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying model can have a radical impact on output. Also we can adjust our output length."
      ],
      "metadata": {
        "id": "37Cmd1WRl3EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this lab, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2\n",
        ")"
      ],
      "metadata": {
        "id": "OzW83NZTl07H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}